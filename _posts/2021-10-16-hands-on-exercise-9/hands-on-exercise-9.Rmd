---
title: "Hands-on Exercise 9"
description: |
  Inhands-on exercise 9, I have learnt how to calibrate geographically weighted regression models by using GWmodel package of R.
author:
  - name: Wong Wei Ling
    url: www.google.com
date: 10-16-2021
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_float: true
    number_sections: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```



# 1 Overview

Geographically weighted regression (GWR) is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable).

In this hands-on exercise, I have learnt to build hedonic pricing models by using GWR methods. The dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational.

# 2 Data

Two data sets will be used in this model building exercise, they are:

- URA Master Plan subzone boundary in shapefile format (i.e. MP14_SUBZONE_WEB_PL)
- condo_resale_2015 in csv format (i.e. condo_resale_2015.csv)

# 3 Import packages

-Geospatial statistical modelling package: **GWmodel**
-Spatial data handling: **sf**
-Attribute data handling: **tidyverse**, especially **read**r, **ggplot2** and **dplyr**
-Choropleth mapping: **tmap**


```{r echo=TRUE, eval=TRUE,  cache=TRUE}
packages = c('olsrr', 'corrplot', 'ggpubr', 'sf', 'spdep', 'GWmodel', 'tmap', 'tidyverse')
for (p in packages){
  if(!require(p, character.only = T)){
    install.packages(p)
  }
  library(p,character.only = T)
}
```

**A short note about GWmodel**

GWmodel package provides a collection of localised spatial statistical methods, namely: GW summary statistics, GW principal components analysis, GW discriminant analysis and various forms of GW regression; some of which are provided in basic and robust (outlier resistant) forms. Commonly, outputs or parameters of the GWmodel are mapped to provide a useful exploratory tool, which can often precede (and direct) a more traditional or sophisticated statistical analysis.

# 4 Geospatial Data Wrangling

# 4.1 Import geospatial data

- Import *MP_SUBZONE_WEB_PL* shapefile by using *st_read()* of **sf** packages.

```{r echo=TRUE, eval=TRUE,  cache=TRUE}
mpsz = st_read(dsn = "data/geospatial", layer = "MP14_SUBZONE_WEB_PL")

```

Results above show that:

- R object used to contain the imported MP14_SUBZONE_WEB_PL shapefile is called mpsz and it is a simple feature object.
- The geometry type is multipolygon. 
- It is also important to note that mpsz simple feature object does not have EPSG information.

# 4.2 Update CRS information

- Update the newly imported mpsz with the correct ESPG code (i.e. 3414)
- Verify  the newly transformed mpsz_svy21.

```{r echo=TRUE, eval=TRUE,  cache=TRUE}
mpsz_svy21 <- st_transform(mpsz, 3414)
st_crs(mpsz_svy21)

```

# 4.3 reveal the extent of mpsz_svy21

- Use *st_bbox()* of **sf** package

```{r echo=TRUE, eval=TRUE,  cache=TRUE}
st_bbox(mpsz_svy21)
```

# 5 Aspatial Data Wrangling

# 5.1 Import the aspatial data

-  Use *read_csv()* function of **readr** package to import condo_resale_2015 into R as a tibble data frame called condo_resale.
-  use *glimpse()* to display the data structure

```{r echo=TRUE, eval=TRUE,  cache=TRUE}
condo_resale <- read_csv("data/aspatial/Condo_resale_2015.csv")
glimpse(condo_resale)

```

- See the data in XCOORD column

```{r echo=TRUE, eval=TRUE,  cache=TRUE}
head(condo_resale$LONGITUDE) 

```

- See the data in YCOORD column

```{r echo=TRUE, eval=TRUE,  cache=TRUE}
head(condo_resale$LATITUDE) 

```

- See summary statsitics of *condo_resale*

```{r echo=TRUE, eval=TRUE,  cache=TRUE}
summary(condo_resale)

```

# 5.2 Convert aspatial data frame into a sf object

- Use *st_as_sf()* of sf packages.
- *st_transform()* of **sf** package is used to convert the coordinates from wgs84 (i.e. crs:4326) to svy21 (i.e. crs=3414).

```{r echo=TRUE, eval=TRUE,  cache=TRUE}
condo_resale.sf <- st_as_sf(condo_resale,
                            coords = c("LONGITUDE", "LATITUDE"),
                            crs=4326) %>%
  st_transform(crs=3414)
head(condo_resale.sf)
```

# 6 Exploratory Data Analysis

## 6.1 EDA using statistical graphics

### 6.1.1 Plot distribution

- Plot the distribution of SELLING_PRICE by using appropriate Exploratory Data Analysis (EDA) 

```{r echo=TRUE, eval=TRUE,  cache=TRUE}
ggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
```

Results above show that: 

- Reveals a right skewed distribution. This means that more condominium units were transacted at relative lower prices.

### 6.1.2 Normalise using Log Transformation

- Statistically, the skewed distribution can be normalised by using log transformation. 
- Use to derive a new variable called **LOG_SELLING_PRICE** by using a log transformation on the variable SELLING_PRICE.
- It is performed using *mutate()* of **dplyr** package.

```{r echo=TRUE, eval=TRUE,  cache=TRUE}
condo_resale.sf <- condo_resale.sf %>%
  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))
```

### 6.1.3 Plot **LOG_SELLING_PRICE** 

```{r echo=TRUE, eval=TRUE,  cache=TRUE}
ggplot(data=condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
```

Notice that the distribution is relatively **less skewed after the transformation**.

## 6.2 Multiple Histogram Plots distribution of variables

- The code chunk below is used to create 12 histograms.
- *ggarrnage()* of **ggpubr** package is used to organised these histogram into a 3 columns by 4 rows small multiple plot.

```{r echo=TRUE, eval=TRUE,  cache=TRUE}
AREA_SQM <- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
AGE <- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")  
PROX_CBD <- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
PROX_CHILDCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
PROX_ELDERLYCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
PROX_URA_GROWTH_AREA <- ggplot(data=condo_resale.sf, aes(x= `PROX_URA_GROWTH_AREA`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
PROX_HAWKER_MARKET <- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
PROX_KINDERGARTEN <- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
PROX_MRT <- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
PROX_PARK <- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
PROX_PRIMARY_SCH <- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
PROX_TOP_PRIMARY_SCH <- ggplot(data=condo_resale.sf, aes(x= `PROX_TOP_PRIMARY_SCH`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

ggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT, PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  ncol = 3, nrow = 4)
```

## 6.3 Drawing Statistical Point Map

-  Reveal the geospatial distribution condominium resale prices in Singapore. The map will be prepared by using tmap package.
- *tmap_mode("view")* to use the interactive mode of tmap
- Then, create an interactive point symbol map

- *tm_dots()* is used instead of *tm_bubbles()*

    + set.zoom.limits argument of tm_view() sets the minimum and maximum zoom level to 11 and 14 respectively.
    
- Lastly, *tmap_mode("plot")* to display plot mode

**Note: Display as plot mode for now**

```{r echo=TRUE, eval=TRUE,  cache=TRUE}
tmap_mode("plot")

tm_shape(mpsz_svy21)+
  tm_polygons() +
tm_shape(condo_resale.sf) +  
  tm_dots(col = "SELLING_PRICE",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14))

tmap_mode("plot")
```

# 7 Hedonic Pricing Modelling in R

## 7.1 Simple Linear Regression Method

### 7.1.1 Build Simple Linear Regression model

- Build a simple linear regression model by using SELLING_PRICE as the dependent variable and AREA_SQM as the independent variable.
- *lm()* returns an object of class “lm” or for multiple responses of class c(“mlm”, “lm”).

```{r echo=TRUE, eval=TRUE,  cache=TRUE}
condo.slr <- lm(formula=SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)

```

### 7.1.2 Show statistical summary

- *summary()* and *anova()* can be used to obtain and print a summary and analysis of variance table of the results.
- The generic accessor functions coefficients, effects, fitted.values and residuals extract various useful features of the value returned by **lm**.

```{r echo=TRUE, eval=TRUE,  cache=TRUE}
summary(condo.slr)

```

Results above show that:

- SELLING_PRICE can be explained by using the formula:

      *y = -258121.1 + 14719x1*

- *R-squared* of **0.4518** reveals that the simple regression model built is able to explain about 45% of the resale prices.
- Since p-value is much smaller than 0.0001, we will **reject the null hypothesis** that mean is a good estimator of SELLING_PRICE. This will allow us to infer that simple linear regression model above is a **good estimator** of SELLING_PRICE.
- The Coefficients: section of the report reveals that the p-values of both the estimates of the **Intercept** and **ARA_SQ88M** are **smaller than 0.001**. In v88iew of this, the null hypothesis of the B0 and B1 are equal to 0 will be rejected. As a result, we will be able to infer that the B0 and B1 are good parameter estimates.

### 7.1.3 Visualise best fit curve

- To visualise the best fit curve on a scatterplot, we can incorporate *lm()* as a method function in **ggplot’s** geometry

```{r echo=TRUE, eval=TRUE,  cache=TRUE}
ggplot(data=condo_resale.sf,  
       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +
  geom_point() +
  geom_smooth(method = lm)
```

## 7.2 Multiple Linear Regression Method

### 7.2.1 Visualise the relationships of the independent variables

Ensure that the independent variables used are not highly correlated to each other. If these highly correlated independent variables are used in building a regression model by mistake, the quality of the model will be compromised. This phenomenon is known as **multicollinearity** in statistics.

Correlation matrix is commonly used to visualise the relationships between the independent variables. Beside the *pairs()* of R, there are many packages that support the display of a correlation matrix. In this section, the **corrplot** package will be used.

- To plot a scatterplot matrix of the relationship between the independent variables in condo_resale data.frame.
- Matrix reorder is very important for mining the hiden structure and patter in the matrix. There are four methods in corrplot (parameter order), namely “**AOE**”, “**FPC**”, “**hclust**”, “**alphabet**”. Alphabet order is used, it orders the variables alphabetically.

```{r echo=TRUE, eval=TRUE,  cache=TRUE, fig.width=8, fig.height=8}
corrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = "AOE",
         tl.pos = "td", tl.cex = 0.5, method = "number", type = "upper")
```

Results above show that:

- **Freehold** is highly correlated to **LEASE_99YEAR**. In view of this, it is wiser to only include either one of them in the subsequent model building. As a result, **LEASE_99YEAR** is excluded in the subsequent model building.

### 7.2.2 Build a hedonic pricing model using multiple linear regression method

#### 7.2.2.1 Calibrate the multiple linear regression model

- Use *lm()* to calibrate the multiple linear regression model.
 
```{r echo=TRUE, eval=TRUE,  cache=TRUE}
condo.mlr <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE  + PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET  + PROX_KINDERGARTEN  + PROX_MRT  + PROX_PARK  + PROX_PRIMARY_SCH + PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL  + PROX_SUPERMARKET + PROX_BUS_STOP  + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data=condo_resale.sf)
summary(condo.mlr)
```

Results above show that:

- Not all the indepent variables are statistically significant. We will revised the model by removing those variables which are not statistically significant.

#### 7.2.2.2 Calibrate the revised model 

```{r echo=TRUE, eval=TRUE,  cache=TRUE}
condo.mlr1 <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE  + PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK  + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL  + PROX_BUS_STOP  + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data=condo_resale.sf)
ols_regress(condo.mlr1)
```

### 7.2.3 Check for multicolinearity

-  **olsrr** provides a collection of very useful methods for building better multiple linear regression models:

    + comprehensive regression output
    + residual diagnostics
    + measures of influence
    + heteroskedasticity tests
    + collinearity diagnostics
    + model fit assessment
    + variable contribution assessment
    + variable selection procedures

- the *ols_vif_tol()* of **olsrr** package is used to test if there are sign of multicollinearity.


```{r echo=TRUE, eval=TRUE,  cache=TRUE}
ols_vif_tol(condo.mlr1)

```


Results above show that:

- There are no sign of multicollinearity among the independent variables since the VIF of the independent variables are less than 10

### 7.2.4 Test for Non-Linearity

- In multiple linear regression, it is important for us to test the assumption that linearity and additivity of the relationship between dependent and independent variables.
- Use *ols_plot_resid_fit()* of **olsrr** package is used to perform linearity assumption test.

```{r echo=TRUE, eval=TRUE,  cache=TRUE}
ols_plot_resid_fit(condo.mlr1)

```

Results above show that:

- Most of the data points are scattered around the 0 line, hence we can safely conclude that the relationships between the dependent variable and independent variables are linear.

### 7.2.5 Test for Normality Assumption

- Use *ols_plot_resid_hist()* of **olsrr** package to perform normality assumption test.

```{r echo=TRUE, eval=TRUE,  cache=TRUE}
ols_plot_resid_hist(condo.mlr1)
```

Results above show that:

- Reveals that the residual of the multiple linear regression model (i.e. condo.mlr1) is resemble normal distribution.


For formal statistical test methods, the *ols_test_normality()* of **olsrr** package can be used as well, 

```{r echo=TRUE, eval=TRUE,  cache=TRUE}
ols_test_normality(condo.mlr1)

```

Results above show that:

- p-values of the four tests are way smaller than the alpha value of 0.05. Hence we will reject the null hypothesis that the residual is NOT resemble normal distribution.

### 7.2.6 Test for Spatial Autocorrelation

The hedonic model we try to build are using geographically referenced attributes, hence it is also important for us to visual the residual of the hedonic pricing model.

In order to perform spatial autocorrelation test, we need to convert **condo_resale.sf** simple into a SpatialPointsDataFrame.

#### 7.2.6.1 Export residual of hedonic pricing model

- export the residual of the hedonic pricing model and save it as a data frame.

```{r echo=TRUE, eval=TRUE,  cache=TRUE}
mlr.output <- as.data.frame(condo.mlr1$residuals)

```

#### 7.2.6.2 Join with **condo_resale.sf** object

-  join the newly created data frame with condo_resale.sf object.

```{r echo=TRUE, eval=TRUE,  cache=TRUE}
condo_resale.res.sf <- cbind(condo_resale.sf, 
                        condo.mlr1$residuals) %>%
rename(`MLR_RES` = `condo.mlr1.residuals`)
```

#### 7.2.6.3 Convert to SpatialPointsDataFrame 

- Convert **condo_resale.res.sf** simple feature object into a SpatialPointsDataFrame because **spdep** package can only process sp conformed spatial data objects

```{r echo=TRUE, eval=TRUE,  cache=TRUE}
condo_resale.sp <- as_Spatial(condo_resale.res.sf)
condo_resale.sp
```

#### 7.2.6.4 Display interactive point symbol map

**Note: it is currently in plot mode**

```{r echo=TRUE, eval=TRUE,  cache=TRUE}
tmap_mode("plot")

tm_shape(mpsz_svy21)+
  tm_polygons(alpha = 0.4) +
tm_shape(condo_resale.res.sf) +  
  tm_dots(col = "MLR_RES",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14))

tmap_mode("plot")
```


Results above show that:

- There is sign of spatial autocorrelation.
- To proof that our observation is indeed true, the Moran’s I test will be performed

#### 7.2.6.5 Compute the distance-based weight matrix

-  Compute the distance-based weight matrix by using *dnearneigh()* function of **spdep**.

```{r echo=TRUE, eval=TRUE,  cache=TRUE}
nb <- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE)
summary(nb)
```

#### 7.2.6.6 Convert to a spatial weights

- *nb2listw()* of **spdep** packge will be used to convert the output neighbours lists (i.e. nb) into a spatial weights

```{r echo=TRUE, eval=TRUE,  cache=TRUE}
nb_lw <- nb2listw(nb, style = 'W')
summary(nb_lw)
```

#### 7.2.6.7 Perform  Moran’s I test for residual spatial autocorrelation

- Use *lm.morantest()* of **spdep** package 

```{r echo=TRUE, eval=TRUE,  cache=TRUE}
lm.morantest(condo.mlr1, nb_lw)

```

Results above show that:

- p-value is less than 0.00000000000000022 which is less than the alpha value of 0.05. Hence, we will reject the null hypothesis that the residuals are randomly distributed.
- Since the Observed Global Moran I = 0.1424418 which is greater than 0, we can infer than the residuals resemble cluster distribution.

# 8 Building Hedonic Pricing Models using GWmodel

Learn how to modelling hedonic pricing using both the fixed and adaptive bandwidth schemes

## 8.1 Build Fixed Bandwidth GWR Model

### 8.1.1 Compute fixed bandwith

- *bw.gwr()* of **GWModel** package is used to determine the optimal fixed bandwidth to use in the model. Notice that the argument adaptive is set to FALSE indicates that we are interested to compute the fixed bandwidth.
- There are two possible approaches  to determine the stopping rule, they are: **CV cross-validation approach** and **AIC corrected (AICc) approach**. We define the stopping rule using approach argument.

```{r echo=TRUE, eval=TRUE,  cache=TRUE}
bw.fixed <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE  + PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK  + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL  + PROX_BUS_STOP  + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data=condo_resale.sp, approach="CV", kernel="gaussian", adaptive=FALSE, longlat=FALSE)

```

Results above show that:

- The recommended bandwidth is 971.3398 metres.

Quiz: Why is it in metre?

- to be answered-

### 8.1.2 GWModel method - fixed bandwith

- To calibrate the gwr model using fixed bandwidth and gaussian kernel.

```{r echo=TRUE, eval=TRUE,  cache=TRUE}
gwr.fixed <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE  + PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK  + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL  + PROX_BUS_STOP  + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data=condo_resale.sp, bw=bw.fixed, kernel = 'gaussian', longlat = FALSE)

```

- The output is saved in a list of class “gwrm


The code below can be used to display the model output.

```{r echo=TRUE, eval=TRUE,  cache=TRUE}
gwr.fixed

```

Results above show that:

- The **adjusted r-square of the gwr** is 0.8430418 which is **significantly better** than the **global multiple linear regression** model of 0.6472.

## 8.2 Build Adaptive Bandwidth GWR Model

Calibrate the gwr-based hedonic pricing model by using adaptive bandwidth approach.

### 8.2.1 Compute the adaptive bandwidth

- Similar to the earlier section, we will first use bw.ger() to determine the recommended data point to use.
- Note: adaptive argument set to TRUE.

```{r echo=TRUE, eval=TRUE,  cache=TRUE}
bw.adaptive <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE  + PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK  + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL  + PROX_BUS_STOP  + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data=condo_resale.sp, approach="CV", kernel="gaussian",
adaptive=TRUE, longlat=FALSE)
```

Results above show that:

- 30 is the recommended data points to be used

### 8.2.2 Construct the adaptive bandwidth gwr model

-  Calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and gaussian kernel 

```{r echo=TRUE, eval=TRUE,  cache=TRUE}
gwr.adaptive <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE  + PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK  + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL  + PROX_BUS_STOP  + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data=condo_resale.sp, bw=bw.adaptive, kernel = 'gaussian', adaptive=TRUE, longlat = FALSE)

```

Display the model output

```{r echo=TRUE, eval=TRUE,  cache=TRUE}
gwr.adaptive
```

Results above show that:

- The **adjusted r-square of the gwr** is 0.8561185 which is **significantly better** than the **global multiple linear regression** model of 0.6472

```{r echo=TRUE, eval=TRUE,  cache=TRUE}

```


```{r echo=TRUE, eval=TRUE,  cache=TRUE}

```


```{r echo=TRUE, eval=TRUE,  cache=TRUE}

```

```{r echo=TRUE, eval=TRUE,  cache=TRUE}

```


```{r echo=TRUE, eval=TRUE,  cache=TRUE}

```


```{r echo=TRUE, eval=TRUE,  cache=TRUE}

```


```{r echo=TRUE, eval=TRUE,  cache=TRUE}

```

```{r echo=TRUE, eval=TRUE,  cache=TRUE}

```


```{r echo=TRUE, eval=TRUE,  cache=TRUE}

```


```{r echo=TRUE, eval=TRUE,  cache=TRUE}

```


```{r echo=TRUE, eval=TRUE,  cache=TRUE}

```

```{r echo=TRUE, eval=TRUE,  cache=TRUE}

```


```{r echo=TRUE, eval=TRUE,  cache=TRUE}

```


```{r echo=TRUE, eval=TRUE,  cache=TRUE}

```


```{r echo=TRUE, eval=TRUE,  cache=TRUE}

```

```{r echo=TRUE, eval=TRUE,  cache=TRUE}

```


```{r echo=TRUE, eval=TRUE,  cache=TRUE}

```



